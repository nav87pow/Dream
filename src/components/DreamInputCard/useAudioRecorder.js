// src/components/DreamInputCard/useAudioRecorder.js
import { useCallback, useRef, useState } from "react";

// ğŸ‘‡ ×‘×¡×™×¡ ×œ-API: ×× ×¡×” REACT_APP_API_URL, ××—×¨×ª ××—×œ×™×˜ ×œ×¤×™ ×“×•××™×™×Ÿ
const API_BASE_URL =
  process.env.REACT_APP_API_URL ||
  (window.location.hostname === "localhost"
    ? "http://localhost:4000"
    : "https://dream-eyyq.onrender.com");

// ×ª×‘× ×™×ª ×–×× ×™ ×”×˜×¨×™×’×¨ ×‘××™×œ×™×©× ×™×•×ª: 5, 10, 20, 30, 40, 50 ×©× ×™×•×ª
const TRIGGER_PATTERN = [5000, 10000, 20000, 30000, 40000, 50000];
// ××—×¨×™ ×©×”×’×¢× ×• ×œ×¤×¢× ×”×¨××©×•× ×” ×©×œ 50 ×©× ×™×•×ª â€“ × ×•×¡×™×£ ×¢×•×“ 50 ×©× ×™×•×ª ×›×œ ×¤×¢×

/**
 * useAudioRecorder
 *
 * - ××§×œ×™×˜ ××•×“×™×• ××”××™×§×¨×•×¤×•×Ÿ ×¢×´×™ MediaRecorder ×¢× timeslice = 1000ms (×©× ×™×”).
 * - ×©×•××¨ ××ª *×›×œ* ×”-chunks ×‘Ö¾chunksRef.
 * - ×‘×–××Ÿ ×”×”×§×œ×˜×” ×™×© ×˜×™×™××¨ ×¤× ×™××™ ×©×‘×•×“×§ ×›×œ ×—×¦×™ ×©× ×™×”:
 *   - ×× ×¢×‘×¨×• 5/10/20/... ×©× ×™×•×ª ×××– ×ª×—×™×œ×ª ×”×”×§×œ×˜×”
 *   - × ×‘× ×” Blob ××›×œ ××” ×©×”×•×§×œ×˜ ×¢×“ ×¢×›×©×™×• ×•×©×•×œ×—×™× ××•×ª×• ×œ×ª××œ×•×œ (transcribeBlob)
 *   - ×”×˜×§×¡×˜ ×”××¦×˜×‘×¨ × ×©××¨ ×‘-sessionTextRef
 *   - × ×©×œ×— ×œ××¢×œ×” ×“×¨×š onTranscriptionChunk â€“ DreamInputCard ×›×‘×¨ ×™×•×“×¢ ×œ×—×‘×¨ ××ª ×–×” ×œ×˜×§×¡×˜ ×”××©×ª××©
 *
 * ×”-API:
 * useAudioRecorder({ onTranscriptionChunk, language })
 * ××—×–×™×¨: { startRecording, pauseRecording, resumeRecording, stopRecording, isRecording, isPaused }
 */

export default function useAudioRecorder(options = {}) {
  const { onTranscriptionChunk, language } = options;

  const mediaRecorderRef = useRef(null);
  const streamRef = useRef(null);
  const chunksRef = useRef([]); // ×›×œ ×”-chunks ××”×ª×—×œ×ª ×”×”×§×œ×˜×”
  const sessionTextRef = useRef(""); // ×˜×§×¡×˜ ××¦×˜×‘×¨ ××›×œ ×”×‘×§×©×•×ª

  const isTranscribingRef = useRef(false); // ×›×“×™ ×œ×× ×•×¢ ×‘×§×©×•×ª ×—×•×¤×¤×•×ª
  const timerIdRef = useRef(null);
  const startTimeRef = useRef(null);
  const nextTriggerTimeRef = useRef(null); // ms ×××– ×ª×—×™×œ×ª ×”×”×§×œ×˜×”
  const triggerIndexRef = useRef(0); // ××™× ×“×§×¡ ×‘×ª×•×š TRIGGER_PATTERN

  const [isRecording, setIsRecording] = useState(false);
  const [isPaused, setIsPaused] = useState(false);

  // --- ×©×œ×™×—×ª Blob ×œ×©×¨×ª (×”×”×§×œ×˜×” ×”××œ××” ×¢×“ ×¢×›×©×™×•) ---
  const transcribeBlob = useCallback(
    async (blob) => {
      if (isTranscribingRef.current) {
        // ×™×© ×›×‘×¨ ×‘×§×©×” ×¨×¦×” â€“ ×œ× × ×©×œ×— ×¢×•×“ ××—×ª ×‘××§×‘×™×œ
        return;
      }

      isTranscribingRef.current = true;

      try {
        const formData = new FormData();
        formData.append("file", blob, "audio.webm");

        if (language) {
          formData.append("language", language);
        }

        const res = await fetch(`${API_BASE_URL}/api/transcribe`, {
          method: "POST",
          body: formData,
        });

        if (!res.ok) {
          console.error(
            "[useAudioRecorder] /api/transcribe failed:",
            res.status
          );
          return;
        }

        const data = await res.json();
        const text = (data && data.text) || "";

        if (!text.trim()) {
          return;
        }

        // ×¦×‘×™×¨×ª ×˜×§×¡×˜ ×”×¡×©×Ÿ
        if (!sessionTextRef.current) {
          sessionTextRef.current = text.trim();
        } else {
          const needsSpace =
            !sessionTextRef.current.endsWith(" ") && !text.startsWith(" ");

          sessionTextRef.current =
            sessionTextRef.current +
            (needsSpace ? " " : "") +
            text.trim();
        }

        if (typeof onTranscriptionChunk === "function") {
          onTranscriptionChunk(sessionTextRef.current);
        }
      } catch (err) {
        console.error("[useAudioRecorder] transcribeBlob error:", err);
      } finally {
        isTranscribingRef.current = false;
      }
    },
    [language, onTranscriptionChunk]
  );

  // --- ×”×ª×—×œ×ª ×˜×™×™××¨ ×˜×¨×™×’×¨×™× (5,10,20,30,40,50 ×•××– ×›×œ 50 ×©× ×™×•×ª) ---
  const startTriggerTimer = useCallback(() => {
    // ××™×¤×•×¡ × ×ª×•× ×™ ×–××Ÿ
    startTimeRef.current = Date.now();
    triggerIndexRef.current = 0;
    nextTriggerTimeRef.current = TRIGGER_PATTERN[0]; // 5 ×©× ×™×•×ª

    // ×× ×”×™×” ×˜×™×™××¨ ×§×•×“× â€“ × × ×§×”
    if (timerIdRef.current) {
      clearInterval(timerIdRef.current);
      timerIdRef.current = null;
    }

    // ×˜×™×™××¨ ×›×œ 500ms â€“ ×œ×‘×“×•×§ ×× ×¢×‘×¨× ×• ××ª ×”×˜×¨×™×’×¨ ×”×‘×
    timerIdRef.current = window.setInterval(() => {
      const recorder = mediaRecorderRef.current;
      if (!recorder || !isRecording) {
        return;
      }

      if (!startTimeRef.current || nextTriggerTimeRef.current == null) {
        return;
      }

      const elapsed = Date.now() - startTimeRef.current; // ms ×××– ×ª×—×™×œ×ª ×”×”×§×œ×˜×”

      // ×× ×¢×‘×¨× ×• ××ª ×–××Ÿ ×”×˜×¨×™×’×¨ â€“ ×©×•×œ×—×™× ××ª ×›×œ ×”×”×§×œ×˜×” ×¢×“ ×¢×›×©×™×•
      if (
        elapsed >= nextTriggerTimeRef.current &&
        chunksRef.current.length > 0 &&
        !isTranscribingRef.current
      ) {
        const blob = new Blob(chunksRef.current, { type: "audio/webm" });
        transcribeBlob(blob);

        // ×¢×“×›×•×Ÿ ×–××Ÿ ×”×˜×¨×™×’×¨ ×”×‘×:
        if (triggerIndexRef.current < TRIGGER_PATTERN.length - 1) {
          // ×¢×•×‘×¨×™× ×œ×˜×¨×™×’×¨ ×”×‘× ×‘×¨×©×™××”
          triggerIndexRef.current += 1;
          nextTriggerTimeRef.current =
            TRIGGER_PATTERN[triggerIndexRef.current];
        } else {
          // ×›×‘×¨ ×”×’×¢× ×• ×œ-50 ×©× ×™×•×ª ×œ×¤×—×•×ª ×¤×¢× ××—×ª â€“
          // ××¢×›×©×™×• ××•×¡×™×¤×™× ×¢×•×“ 50 ×©× ×™×•×ª ×‘×›×œ ×¤×¢× (50,100,150,...)
          nextTriggerTimeRef.current += 50000;
        }
      }
    }, 500);
  }, [isRecording, transcribeBlob]);

  const stopTriggerTimer = useCallback(() => {
    if (timerIdRef.current) {
      clearInterval(timerIdRef.current);
      timerIdRef.current = null;
    }
  }, []);

  // --- ×”×ª×—×œ×ª ×”×§×œ×˜×” ---
  const startRecording = useCallback(async () => {
    if (isRecording) return;

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;

      const recorder = new MediaRecorder(stream, {
        mimeType: "audio/webm",
      });

      chunksRef.current = []; // ××™×¤×•×¡
      sessionTextRef.current = ""; // ×˜×§×¡×˜ ××¦×˜×‘×¨ ×—×“×© ×œ×¡×©×Ÿ
      isTranscribingRef.current = false;

      recorder.ondataavailable = (event) => {
        if (event.data && event.data.size > 0) {
          chunksRef.current.push(event.data);
        }
      };

      recorder.onerror = (err) => {
        console.error("[useAudioRecorder] MediaRecorder error:", err);
      };

      // ×›×©×¢×•×¦×¨×™× ××ª ×”×”×§×œ×˜×”, × × ×¡×” ×œ×©×œ×•×— ×¤×¢× ××—×¨×•× ×” ××ª ×”×”×§×œ×˜×” ×”××œ××” (×× ××™×Ÿ ×ª××œ×•×œ ×¨×¥)
      recorder.onstop = async () => {
        stopTriggerTimer();

        if (chunksRef.current.length && !isTranscribingRef.current) {
          const blob = new Blob(chunksRef.current, { type: "audio/webm" });
          await transcribeBlob(blob);
        }
      };

      mediaRecorderRef.current = recorder;
      recorder.start(1000); // ×›×œ ×©× ×™×” × ×§×‘×œ chunk ×§×˜×Ÿ ×œ-chunksRef

      setIsRecording(true);
      setIsPaused(false);

      // ××¤×¢×™×œ×™× ××ª ×œ×•×’×™×§×ª ×”×˜×¨×™×’×¨×™× 5/10/20/30/40/50...
      startTriggerTimer();
    } catch (err) {
      console.error("[useAudioRecorder] getUserMedia error:", err);
    }
  }, [isRecording, startTriggerTimer, stopTriggerTimer, transcribeBlob]);

  // --- Pause ---
  const pauseRecording = useCallback(() => {
    const recorder = mediaRecorderRef.current;
    if (!recorder || !isRecording || isPaused) return;

    try {
      if (typeof recorder.pause === "function") {
        recorder.pause();
      } else {
        recorder.stop();
      }
      stopTriggerTimer();
      setIsPaused(true);
      setIsRecording(false);
    } catch (err) {
      console.error("[useAudioRecorder] pause error:", err);
    }
  }, [isRecording, isPaused, stopTriggerTimer]);

  // --- Resume ---
  const resumeRecording = useCallback(async () => {
    const recorder = mediaRecorderRef.current;

    if (recorder && recorder.state === "paused") {
      try {
        recorder.resume();
        setIsPaused(false);
        setIsRecording(true);

        // ×›×©×××©×™×›×™×, ××ª×—×™×œ×™× ×©×•×‘ ××ª ×“×¤×•×¡ ×”×˜×¨×™×’×¨×™× ××”×ª×—×œ×”
        startTriggerTimer();
        return;
      } catch (err) {
        console.error("[useAudioRecorder] resume error:", err);
      }
    }

    // fallback: ×× ××™×Ÿ recorder ×¤×¢×™×œ â€“ ××ª×—×™×œ×™× ×”×§×œ×˜×” ×—×“×©×”
    if (!isRecording) {
      await startRecording();
    } else {
      setIsPaused(false);
    }
  }, [isRecording, startRecording, startTriggerTimer]);

  // --- Stop ×œ×’××¨×™ ---
  const stopRecording = useCallback(() => {
    const recorder = mediaRecorderRef.current;

    try {
      if (recorder && recorder.state !== "inactive") {
        recorder.stop(); // onstop ×™×“××’ ×œ×©×œ×™×—×” ××—×¨×•× ×” ×× ×¦×¨×™×š
      }
    } catch (err) {
      console.error("[useAudioRecorder] stop error:", err);
    } finally {
      mediaRecorderRef.current = null;
      setIsRecording(false);
      setIsPaused(false);
      isTranscribingRef.current = false;
      stopTriggerTimer();

      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
        streamRef.current = null;
      }
    }
  }, [stopTriggerTimer]);

  return {
    startRecording,
    pauseRecording,
    resumeRecording,
    stopRecording,
    isRecording,
    isPaused,
  };
}
