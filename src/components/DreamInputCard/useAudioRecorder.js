// src/components/DreamInputCard/useAudioRecorder.js
import { useCallback, useRef, useState } from "react";

// ğŸ‘‡ ×‘×¡×™×¡ ×œ-API: ×× ×¡×” REACT_APP_API_URL, ××—×¨×ª ××—×œ×™×˜ ×œ×¤×™ ×“×•××™×™×Ÿ
const API_BASE_URL =
  process.env.REACT_APP_API_URL ||
  (window.location.hostname === "localhost"
    ? "http://localhost:4000"
    : "https://dream-eyyq.onrender.com");

/**
 * useAudioRecorder
 *
 * ××§×œ×™×˜ ××•×“×™×• ××”××™×§×¨×•×¤×•×Ÿ, ×•×©×•×œ×— ×›×œ 5 ×©× ×™×•×ª chunk ×œ×©×¨×ª ×”×ª××œ×•×œ.
 * ×›×œ ×ª×©×•×‘×” ××”×©×¨×ª ××¦×˜×‘×¨×ª ×œ-sessionTextRef ×•× ×©×œ×—×ª ×œ××¢×œ×” ×“×¨×š onTranscriptionChunk.
 *
 * ×”-API:
 * useAudioRecorder({ onTranscriptionChunk, language })
 * ××—×–×™×¨: { startRecording, pauseRecording, resumeRecording, stopRecording, isRecording, isPaused }
 */

export default function useAudioRecorder(options = {}) {
  const { onTranscriptionChunk, language } = options;

  const mediaRecorderRef = useRef(null);
  const streamRef = useRef(null);

  // ×˜×§×¡×˜ ××¦×˜×‘×¨ ×©×œ ×”×”×§×œ×˜×” ×”× ×•×›×—×™×ª
  const sessionTextRef = useRef("");

  // ×“×’×œ ×œ×× ×™×¢×ª ×‘×§×©×•×ª ×—×•×¤×¤×•×ª ×œ×©×¨×ª
  const isTranscribingRef = useRef(false);

  const [isRecording, setIsRecording] = useState(false);
  const [isPaused, setIsPaused] = useState(false);

  // --- ×©×œ×™×—×ª chunk ×™×—×™×“ ×œ×©×¨×ª ---
  const transcribeChunk = useCallback(
    async (blob) => {
      if (isTranscribingRef.current) {
        // ×›×‘×¨ ×™×© ×‘×§×©×” ×¨×¦×” â€“ ×›×“×™ ×œ× ×œ×”×¤×¦×™×¥ ××ª ×”×©×¨×ª, × ×“×œ×’ ×¢×œ ×”-chunk ×”×–×”
        return;
      }

      isTranscribingRef.current = true;

      try {
        const formData = new FormData();
        formData.append("file", blob, "chunk.webm");

        if (language) {
          formData.append("language", language);
        }

        const res = await fetch(`${API_BASE_URL}/api/transcribe`, {
          method: "POST",
          body: formData,
        });

        if (!res.ok) {
          console.error(
            "[useAudioRecorder] /api/transcribe failed:",
            res.status
          );
          return;
        }

        const data = await res.json();
        const text = (data && data.text) || "";

        if (!text.trim()) {
          return;
        }

        // ×¦×‘×™×¨×ª ×˜×§×¡×˜ ×”×¡×©×Ÿ
        if (!sessionTextRef.current) {
          sessionTextRef.current = text.trim();
        } else {
          const needsSpace =
            !sessionTextRef.current.endsWith(" ") && !text.startsWith(" ");

          sessionTextRef.current =
            sessionTextRef.current +
            (needsSpace ? " " : "") +
            text.trim();
        }

        if (typeof onTranscriptionChunk === "function") {
          onTranscriptionChunk(sessionTextRef.current);
        }
      } catch (err) {
        console.error("[useAudioRecorder] transcribeChunk error:", err);
      } finally {
        isTranscribingRef.current = false;
      }
    },
    [language, onTranscriptionChunk]
  );

  // --- ×”×ª×—×œ×ª ×”×§×œ×˜×” ---
  const startRecording = useCallback(async () => {
    if (isRecording) return;

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;

      const recorder = new MediaRecorder(stream, {
        mimeType: "audio/webm",
      });

      // ××™×¤×•×¡ ×˜×§×¡×˜ ×”×¡×©×Ÿ ×‘×ª×—×™×œ×ª ×”×§×œ×˜×” ×—×“×©×”
      sessionTextRef.current = "";

      recorder.ondataavailable = async (event) => {
        // × ×§×¨× ×›×œ 5 ×©× ×™×•×ª
        if (!event.data || event.data.size === 0) return;
        await transcribeChunk(event.data);
      };

      recorder.onerror = (err) => {
        console.error("[useAudioRecorder] MediaRecorder error:", err);
      };

      mediaRecorderRef.current = recorder;
      recorder.start(5000); // ğŸ‘ˆ ×›×œ 5000ms × ×§×‘×œ ondataavailable

      setIsRecording(true);
      setIsPaused(false);
    } catch (err) {
      console.error("[useAudioRecorder] getUserMedia error:", err);
    }
  }, [isRecording, transcribeChunk]);

  // --- Pause ---
  const pauseRecording = useCallback(() => {
    const recorder = mediaRecorderRef.current;
    if (!recorder || !isRecording || isPaused) return;

    try {
      if (typeof recorder.pause === "function") {
        recorder.pause();
      } else {
        // fallback: ×× ××™×Ÿ pause, × ×¢×¦×•×¨
        recorder.stop();
      }
      setIsPaused(true);
    } catch (err) {
      console.error("[useAudioRecorder] pause error:", err);
    }
  }, [isRecording, isPaused]);

  // --- Resume ---
  const resumeRecording = useCallback(async () => {
    const recorder = mediaRecorderRef.current;

    if (recorder && recorder.state === "paused") {
      try {
        recorder.resume();
        setIsPaused(false);
        return;
      } catch (err) {
        console.error("[useAudioRecorder] resume error:", err);
      }
    }

    // fallback: ×× ××™×Ÿ recorder ×¤×¢×™×œ â€“ ××ª×—×™×œ×™× ×”×§×œ×˜×” ×—×“×©×”
    if (!isRecording) {
      await startRecording();
    } else {
      setIsPaused(false);
    }
  }, [isRecording, startRecording]);

  // --- Stop ×œ×’××¨×™ ---
  const stopRecording = useCallback(() => {
    const recorder = mediaRecorderRef.current;

    try {
      if (recorder && recorder.state !== "inactive") {
        recorder.stop();
      }
    } catch (err) {
      console.error("[useAudioRecorder] stop error:", err);
    } finally {
      mediaRecorderRef.current = null;
      setIsRecording(false);
      setIsPaused(false);
      isTranscribingRef.current = false;

      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
        streamRef.current = null;
      }
      // ×œ× ×××¤×¡×™× ×›××Ÿ sessionTextRef.current â€“ ×”×˜×§×¡×˜ ×›×‘×¨ × ×©×œ×— ×œ××¢×œ×”
    }
  }, []);

  return {
    startRecording,
    pauseRecording,
    resumeRecording,
    stopRecording,
    isRecording,
    isPaused,
  };
}
